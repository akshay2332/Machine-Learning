{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Descent\n",
      "\n",
      "Recall: 90.00%\n",
      "Precision: 29.16%\n",
      "Accuracy: 31.09%\n",
      "\n",
      "Mini-Batch Gradient Descent\n",
      "\n",
      "Recall: 0.00%\n",
      "Precision: 0.00%\n",
      "Accuracy: 62.77%\n"
     ]
    }
   ],
   "source": [
    "column_list = ['id','category']\n",
    "for i in range(0,30):\n",
    "    attr_label = 'attr' + str(i+1)\n",
    "    column_list.append(attr_label)\n",
    "\n",
    "data = pd.read_csv(\"wdbc.data\" ,  sep=\",\", names=column_list)\n",
    "\n",
    "classes = data['category']\n",
    "y_data = pd.Series(np.where(classes =='M', 1, 0))\n",
    "\n",
    "X_data = data.drop(['id','category'],axis=1)\n",
    "\n",
    "\n",
    "def initializeW(dimensions, gradient_type):\n",
    "    if \"SDG\" == gradient_type:\n",
    "        w = [0]*(dimensions) \n",
    "    else:\n",
    "        w = np.zeros(shape=(dimensions, 1))\n",
    "    return w\n",
    "\n",
    "def signmoid_funtion(a):\n",
    "    sigmoid_value = 0\n",
    "    sigmoid_value = 1 / (1 + np.exp(-a))\n",
    "    \n",
    "    return sigmoid_value\n",
    "\n",
    "    \n",
    "# Make a prediction with coefficients\n",
    "def predict_class(row, coefficients):\n",
    "    prediction = coefficients[0]\n",
    "    \n",
    "    for i in range(1, len(row)-1):\n",
    "        prediction += coefficients[i + 1] * row[i]\n",
    "\n",
    "    return signmoid_funtion(prediction)\n",
    "\n",
    "\n",
    "def predict_class_MBG(X_test, coefficients):\n",
    "    return signmoid_funtion(np.dot(X_test, coefficients))\n",
    "\n",
    "def evaluate_gradient_SDG(learning_rate, actual_value, prediction):\n",
    "    gradient_value = learning_rate * (actual_value - prediction) * prediction * (1.0 - prediction)\n",
    "    return gradient_value \n",
    "\n",
    "def evaluate_gradient_MBG(learning_rate, input_batch, prediction,weights):\n",
    "    \n",
    "    gradient_value = np.dot(input_batch.T, \\\n",
    "                            (signmoid_funtion(np.dot(input_batch, weights)) \\\n",
    "                             - prediction)) /prediction.shape[0]\n",
    "    return (learning_rate * gradient_value) \n",
    "\n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "def find_coefficients_SDG(train_X, predictions, learning_rate, n_epoch, w_vector):\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        for index, row in train_X.iterrows():\n",
    "            prediction_of_class = 0\n",
    "            row_values = row.values\n",
    "            real_value = predictions.iloc[index].values[0] \n",
    "        \n",
    "            prediction_of_class = predict_class(row_values, w_vector)\n",
    "    \n",
    "            gradient = evaluate_gradient_SDG(learning_rate = learning_rate,\\\n",
    "                                         actual_value = real_value, prediction = prediction_of_class)\n",
    "            w_vector[0] = w_vector[0] + gradient\n",
    "            for i in range(len(row_values)-1):\n",
    "                w_vector[i+1] = w_vector[i+1] + gradient * row_values[i]\n",
    "    \n",
    "    return w_vector\n",
    "\n",
    "def create_mini_batches(X_data,y_data,batch_size = 32 ): \n",
    "    mini_batches_list = [] \n",
    "    \n",
    "    merged_data = np.hstack((X_data, y_data)) \n",
    "    np.random.shuffle(merged_data) \n",
    "    \n",
    "    limit_of_batch = merged_data.shape[0] \n",
    "    \n",
    "    i = 0 \n",
    "    for i in range(limit_of_batch - (batch_size + 1)):\n",
    "        mini_batch = merged_data[i * batch_size:(i + 1)*batch_size, :] \n",
    "        X_mini_batch = mini_batch[:, :-1] \n",
    "        Y_mini_batch = mini_batch[:, -1].reshape((-1, 1)) \n",
    "        mini_batches_list.append((X_mini_batch, Y_mini_batch))\n",
    "        \n",
    "    return mini_batches_list \n",
    "    \n",
    "    \n",
    "\n",
    "def find_coefficients_MBG(train_X, predictions, learning_rate, n_epoch, w_vector):\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        batches = create_mini_batches(X_data=train_X,y_data=predictions)\n",
    "        \n",
    "        for batch in batches: \n",
    "            X_batch, y_batch = batch\n",
    "\n",
    "            if X_batch.shape[0] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                w_vector = w_vector - evaluate_gradient_MBG(learning_rate = learning_rate,\\\n",
    "                                         input_batch = X_batch, prediction = y_batch , weights = w_vector) \n",
    "    return w_vector\n",
    "            \n",
    "    \n",
    "\n",
    "def normalize_input_data(X_data):\n",
    "    X_data = (X_data-X_data.min())/(X_data.max()-X_data.min())\n",
    "    return X_data\n",
    "\n",
    "\n",
    "def perform_cross_validation(gradient_type , kfold_factor, X_data, predictions):\n",
    "    \n",
    "    k_fold = KFold(n_splits=kfold_factor)\n",
    "    X = np.array(X_data)\n",
    "    X_shuffled , predictions_shuffled = shuffle(X, predictions, random_state=0)\n",
    "    accuracy_list = []\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "   \n",
    "    for train_index, test_index in k_fold.split(X_shuffled):\n",
    "        \n",
    "        w = initializeW(X_data.shape[1] ,gradient_type)\n",
    "    \n",
    "        X_train, X_test = X_shuffled[train_index], X_shuffled[test_index]\n",
    "        \n",
    "        y_train, y_test = predictions_shuffled[train_index], predictions_shuffled[test_index]\n",
    "        \n",
    "        X_train = pd.DataFrame(data=X_train)\n",
    "        y_train = pd.DataFrame(data=y_train)\n",
    "        \n",
    "            \n",
    "        test_predictions=[]\n",
    "        actual_predicitions=[]\n",
    "        \n",
    "        if \"SGD\" == gradient_type:\n",
    "            coefficients = find_coefficients_SDG(train_X = X_train, predictions = y_train, learning_rate = 0.1,\\\n",
    "                               n_epoch = 100 , w_vector = w)\n",
    "            \n",
    "            for row in X_test:\n",
    "                prediction_class = predict_class(row, coefficients)\n",
    "                \n",
    "                test_predictions.append(prediction_class)\n",
    "        else:\n",
    "            coefficients = find_coefficients_MBG(train_X = X_train, predictions = y_train, learning_rate = 0.1,\\\n",
    "                               n_epoch = 100 , w_vector = w)\n",
    "            y_pred = predict_class_MBG(X_test, coefficients)\n",
    "            test_predictions =y_pred\n",
    "                \n",
    "        counter = 0\n",
    "        truePositive = 0\n",
    "        trueNegative = 0\n",
    "        falseNegative = 0\n",
    "        falsePositive = 0\n",
    "        \n",
    "        for value in y_test:\n",
    "            pred_value = int(test_predictions[counter])\n",
    "            \n",
    "            if pred_value == 1 and value == 1:\n",
    "                truePositive += 1\n",
    "            elif pred_value == 0 and value == 0:\n",
    "                trueNegative += 1\n",
    "            elif pred_value == 0 and value == 1:\n",
    "                falseNegative += 1\n",
    "            elif pred_value == 1 and value == 0 :\n",
    "                falsePositive += 1\n",
    "                \n",
    "            counter = counter + 1 \n",
    "        \n",
    "        recall = truePositive / ( truePositive + falseNegative if truePositive + falseNegative else 1) * 100\n",
    "        accuracy = (truePositive + trueNegative) / ((truePositive + falsePositive + trueNegative +falseNegative)\\\n",
    "                                                    if (truePositive + falsePositive + trueNegative +falseNegative) else 1) * 100\n",
    "        precision = truePositive / (truePositive + falsePositive if truePositive + falsePositive else 1) * 100\n",
    "        \n",
    "        accuracy_list.append(accuracy)\n",
    "        recall_list.append(recall)\n",
    "        precision_list.append(precision)\n",
    "        \n",
    "    print(\"Recall: %.2f\" % np.average(recall_list)+str('%'))\n",
    "    print(\"Precision: %.2f\"% np.average(precision_list)+str('%'))\n",
    "    print(\"Accuracy: %.2f\" %np.average(accuracy_list)+str('%'))\n",
    "        \n",
    "X_data_norm = normalize_input_data(X_data)\n",
    "print(\"Stochastic Gradient Descent\\n\")\n",
    "perform_cross_validation(\"SGD\" , 10, X_data_norm, y_data)\n",
    "\n",
    "print(\"\\nMini-Batch Gradient Descent\\n\")\n",
    "perform_cross_validation(\"MBG\" , 10, X_data_norm, y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
